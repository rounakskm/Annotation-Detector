{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"annotation_detector.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"2LM-8KRhguoG","colab_type":"text"},"cell_type":"markdown","source":["# CNN to detect and localize annotations in a given image file."]},{"metadata":{"id":"SUWPuk7-jbMS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":126},"outputId":"99415e2c-c77a-400b-c4f5-2a79534c885b","executionInfo":{"status":"ok","timestamp":1534974713175,"user_tz":240,"elapsed":8257,"user":{"displayName":"Soumya Mohanty","photoUrl":"//lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s50-c-k-no/photo.jpg","userId":"105930513908317089321"}}},"cell_type":"code","source":["# Code to check the resources available Ex. GPU RAM\n","'''\n","# memory footprint support libraries/code\n","!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip install gputil\n","!pip install psutil\n","!pip install humanize\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","GPUs = GPU.getGPUs()\n","# XXX: only one GPU on Colab and isn’t guaranteed\n","gpu = GPUs[0]\n","def printm():\n"," process = psutil.Process(os.getpid())\n"," print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n"," print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","printm()\n","'''\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.3.0)\r\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gputil) (1.14.5)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.7)\n","Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n","Gen RAM Free: 12.7 GB  | Proc size: 138.7 MB\n","GPU RAM Free: 11438MB | Used: 1MB | Util   0% | Total 11439MB\n"],"name":"stdout"}]},{"metadata":{"id":"O-wv6Hc5jtVX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":345},"outputId":"2f43be7b-72ee-42fa-b966-4754badc1eee","executionInfo":{"status":"ok","timestamp":1534972755762,"user_tz":240,"elapsed":3844,"user":{"displayName":"Soumya Mohanty","photoUrl":"//lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s50-c-k-no/photo.jpg","userId":"105930513908317089321"}}},"cell_type":"code","source":["# Installing PyDrive\n","!pip install PyDrive"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting PyDrive\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n","\u001b[K    100% |████████████████████████████████| 993kB 7.4MB/s \n","\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n","Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.2)\n","Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n","Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.11.0)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n","Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.2)\n","Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (3.4.2)\n","Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.4)\n","Building wheels for collected packages: PyDrive\n","  Running setup.py bdist_wheel for PyDrive ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n","Successfully built PyDrive\n","Installing collected packages: PyDrive\n","Successfully installed PyDrive-1.3.1\n"],"name":"stdout"}]},{"metadata":{"id":"DYrKy1RrjuKC","colab_type":"code","colab":{}},"cell_type":"code","source":["from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once in a notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sJJDh3BVjfly","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"918440c9-e012-4503-9727-2fd6ca4b724b","executionInfo":{"status":"ok","timestamp":1534972786708,"user_tz":240,"elapsed":1497,"user":{"displayName":"Soumya Mohanty","photoUrl":"//lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s50-c-k-no/photo.jpg","userId":"105930513908317089321"}}},"cell_type":"code","source":["# Insert zip folder with all training images in it. Then unzip it\n","fileId = drive.CreateFile({'id': '19kmV8qjzVzUj02TwHS1Uw7BzdB63dF_-'}) # id is the id of the file being uploaded\n","print (fileId['title'])  # dataset_train.zip\n","file = fileId['title']\n","fileId.GetContentFile(file)  # Save Drive file as a local file\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["dataset_train.zip\n"],"name":"stdout"}]},{"metadata":{"id":"SkKuZ_5dkItS","colab_type":"code","colab":{}},"cell_type":"code","source":["# Unzip the datafile\n","!unzip dataset_train.zip -d ./"],"execution_count":0,"outputs":[]},{"metadata":{"id":"r2ZdbGuTkMpm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"0ce61b9f-5b48-4364-a2bd-5b079de8310f","executionInfo":{"status":"ok","timestamp":1534978723996,"user_tz":240,"elapsed":3606,"user":{"displayName":"Soumya Mohanty","photoUrl":"//lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s50-c-k-no/photo.jpg","userId":"105930513908317089321"}}},"cell_type":"code","source":["!ls\n","!rm dataset_train.zip\n","!ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["adc.json  datalab  dataset_test  dataset_train\tsample_data\n","rm: cannot remove 'dataset_train.zip': No such file or directory\n","adc.json  datalab  dataset_test  dataset_train\tsample_data\n"],"name":"stdout"}]},{"metadata":{"id":"-7eypTHskY9J","colab_type":"text"},"cell_type":"markdown","source":["# Installing PyTorch & OpenCV"]},{"metadata":{"id":"1VccBVaakSDa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"b71868d5-08af-4650-d66a-27a015d062c4","executionInfo":{"status":"ok","timestamp":1534972852412,"user_tz":240,"elapsed":39343,"user":{"displayName":"Soumya Mohanty","photoUrl":"//lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s50-c-k-no/photo.jpg","userId":"105930513908317089321"}}},"cell_type":"code","source":["# Install pytorch\n","# http://pytorch.org/\n","from os import path\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","\n","accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision"],"execution_count":7,"outputs":[{"output_type":"stream","text":["tcmalloc: large alloc 1073750016 bytes == 0x5bf3c000 @  0x7f847e34e1c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\r\n"],"name":"stdout"}]},{"metadata":{"id":"1mmkyWTCkeh8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"c9dccb20-ae68-4d38-8841-3ee49bcdb8e9","executionInfo":{"status":"ok","timestamp":1534978726892,"user_tz":240,"elapsed":615,"user":{"displayName":"Soumya Mohanty","photoUrl":"//lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s50-c-k-no/photo.jpg","userId":"105930513908317089321"}}},"cell_type":"code","source":["#Check install and version\n","import torch\n","torch.__version__\n","\n","# Check cuda avaialability\n","if torch.cuda.is_available():\n","   print(\"Yay!!\")\n","    "],"execution_count":2,"outputs":[{"output_type":"stream","text":["Yay!!\n"],"name":"stdout"}]},{"metadata":{"id":"sRV5IPnVknoz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":363},"outputId":"630f9d6a-75e6-4391-c9bb-4f051cb6efdb","executionInfo":{"status":"ok","timestamp":1534972875539,"user_tz":240,"elapsed":15823,"user":{"displayName":"Soumya Mohanty","photoUrl":"//lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s50-c-k-no/photo.jpg","userId":"105930513908317089321"}}},"cell_type":"code","source":["# Install openCV\n","!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python\n","\n","# Installing Pillow as py torch transform dependencies\n","!pip install Pillow==4.1.1\n","!pip install image"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Collecting Pillow==4.1.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/e5/88b3d60924a3f8476fa74ec086f5fbaba56dd6cee0d82845f883b6b6dd18/Pillow-4.1.1-cp36-cp36m-manylinux1_x86_64.whl (5.7MB)\n","\u001b[K    100% |████████████████████████████████| 5.7MB 4.5MB/s \n","\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.1.1) (0.45.1)\n","Installing collected packages: Pillow\n","  Found existing installation: Pillow 5.2.0\n","    Uninstalling Pillow-5.2.0:\n","      Successfully uninstalled Pillow-5.2.0\n","Successfully installed Pillow-4.1.1\n","Collecting image\n","  Downloading https://files.pythonhosted.org/packages/a7/5b/c0358fe83daab29070e38a945809585f1fd6353e65b687f602688f32c3c2/image-1.5.24-py2.py3-none-any.whl\n","Collecting django (from image)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/1a/e0ac7886c7123a03814178d7517dc822af0fe51a72e1a6bff26153103322/Django-2.1-py3-none-any.whl (7.3MB)\n","\u001b[K    100% |████████████████████████████████| 7.3MB 4.0MB/s \n","\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from image) (4.1.1)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from django->image) (2018.5)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->image) (0.45.1)\n","Installing collected packages: django, image\n","Successfully installed django-2.1 image-1.5.24\n"],"name":"stdout"}]},{"metadata":{"id":"7RZPeo2ukrfU","colab_type":"code","colab":{}},"cell_type":"code","source":["import os\n","import cv2\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision\n","import torch.utils.data as data\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","\n","from PIL import Image\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YNxyfpb9kwwb","colab_type":"code","colab":{}},"cell_type":"code","source":["# Custom Dataset created to load training and testing data\n","\n","class ImgDataset(Dataset):\n","    def __init__(self, data_dir, transform = None, \n","                 img_shape = (600,600), grid_dim = (30,30)):\n","        \n","        # Length of per grid vector, based on number of classes being detected\n","        self.vector_len = 5 \n","        \n","        self.img_shape = img_shape\n","        self.grid_dim = grid_dim\n","        self.transform = transform\n","        \n","        \n","        # Create a grid. Grid doesnt change as image size is same\n","        self.grid_list, self.col_width, self.row_height = \\\n","        self.create_grid(self.img_shape, self.grid_dim)\n","        \n","        # Fetch all files from dataset directory\n","        self.files = os.listdir(data_dir)\n","        self.files = [os.path.join(data_dir,f) for f in self.files]\n","\n","        # Make separate list for images and text files\n","        self.img_list = [f for f in self.files if '.jpg' in f]\n","        self.txt_list = [f for f in self.files if '.txt' in f]\n","        self.labels = []\n","        \n","        for txt_file in self.txt_list:\n","            # Call function to iterate over grid_list \n","            # and generate label vector for the image\n","            img_label = self.label_vector_gen(txt_file)\n","            # Has one-to-one correspondence with img_list\n","            self.labels.append(img_label) \n","        \n","\n","    def __len__(self):\n","        '''Return size of the dataset'''\n","        return len(self.img_list)\n","    \n","    def __getitem__(self,idx):\n","        '''Enable indexing on dataset'''\n","        # Processing the image\n","        img = Image.open(self.img_list[idx]).convert('RGB')  # PIL image\n","        image = self.transform(img)\n","        \n","        # Processing the label for that corresponding image\n","        label = self.labels[idx]\n","        label = torch.FloatTensor(label)\n","        # Flatenning the label into [4500] size vector\n","        label = label.view(-1)\n","        return image, label\n","\n","    def create_grid(self,img_shape,grid_dim):\n","        '''\n","        Function: \n","            Function creates a grid with cells of equal size.\n","        \n","        Parameters:\n","            img_shape : dimensions of the image (height,width).\n","            grid_dim  : dimensions of the grid (rows,colms).\n","            Note: width must be divisible by no. of columns\n","            and height must be divisible by no. of rows\n","            \n","        Returns:\n","            This function returns a list of top left coordinates of cells, \n","            along with the col_width and row_height.\n","        '''\n","        height, width = img_shape\n","        rows,colms = grid_dim\n","        \n","        col_width = width//colms\n","        \n","        row_height = height//rows\n","        \n","        grid_list = []\n","        \n","        for c in range(0,width,col_width): # Step size is the col_width\n","            for r in range(0,height,row_height): # Step size is the row_height\n","                grid_list.append((c,r))\n","                \n","        return grid_list, col_width, row_height\n","\n","\n","    def label_vector_gen(self,txt_file):\n","        '''\n","        Function: \n","            Function generates the label vector for each image.\n","        \n","        Parameters:\n","            txt_file  : Name of file name that has the bounding box data.\n","        Returns:\n","            This function returns the label vector for each image.\n","            Note: Each label is of size\n","            grid_rows x grid_colmn x no.of classes x len of vector\n","            = len(grid_list) x 1 x 5\n","        '''\n","        \n","        # Open file to read box details\n","        with open(txt_file, 'r') as file:\n","            bounding_boxes = file.read().split('\\n')\n","            # Remove last emelent as it is empty\n","            bounding_boxes = bounding_boxes[:-1]\n","        \n","        coordinate_list = [] # Store coordinates of cells with bbox center \n","        label = [] # Store the labels for each bounding box\n","        \n","        if len(bounding_boxes)>0:\n","            for box in bounding_boxes:\n","                box = box.split(' ')\n","                # Each element of box list has one attribute of the box as str\n","                # un-normalize them by multiplying width and height\n","                box_center_x = float(box[1]) * self.img_shape[1]\n","                box_center_y = float(box[2]) * self.img_shape[0]\n","                \n","                # Get the coordinate of the cell containing the box centers\n","                grid_x = (box_center_x//self.col_width) * self.col_width\n","                grid_y = (box_center_y//self.row_height) * self.row_height\n","                coordinate_list.append((grid_x,grid_y))\n","                \n","                \n","            # Check if point lies in cell and generate the vector\n","            for cell in self.grid_list:\n","                if cell in coordinate_list:\n","                    # Make vector as required\n","                    idx = coordinate_list.index(cell)\n","                    box_info = bounding_boxes[idx] # Get corresponding box data\n","                    box_info = box_info.split(' ')\n","                    \n","                    class_pred = float(box_info[0]) \n","                    x_center = float(box_info[1])\n","                    y_center = float(box_info[2])\n","                    width = float(box_info[3])\n","                    height = float(box_info[4])\n","                    \n","                    label.append((class_pred, \n","                                  x_center, \n","                                  y_center, \n","                                  width, \n","                                  height))\n","                else:\n","                    # 99 means no value as object doesnt exist in the grid\n","                    # (class_pred, x_center, y_center, width, height)\n","                    label.append((99,99,99,99,99))\n","                    \n","        elif len(bounding_boxes) == 0:\n","            label = [(99,99,99,99,99)] * (self.grid_dim[0] * self.grid_dim[1])\n","            \n","        return label"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vb7rvSh3k9fM","colab_type":"code","colab":{}},"cell_type":"code","source":["# Loading trainig data\n","\n","# Normalizing the images\n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n","                                 std=[0.229, 0.224, 0.225])\n","\n","# creating transform, resizing again (can be removed)\n","transform = transforms.Compose([transforms.Resize((600,600)),\n","                                transforms.ToTensor(),\n","                                normalize])\n","\n","# Creating dataset object\n","train_dataset = ImgDataset('dataset_train', transform = transform, \n","                 img_shape = (600,600), grid_dim = (30,30))\n","\n","# Creating dataloader to load a batch of 4 images at once\n","train_dataloader = DataLoader(train_dataset,\n","                              batch_size=8, \n","                              shuffle=True,\n","                              num_workers=0,\n","                              pin_memory=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BTCIrY3M7nlg","colab_type":"code","colab":{}},"cell_type":"code","source":["# Create Convolutional Neural Net to detect and localize annotations\n","# Net should take an image as input and give out a vector equivalent \n","# to the label vector as output.\n","# Input image shape (h=600,w=600,c=3) in batch: 4x3x600x300\n","\n","# Net should give 900x5 output as of now\n","# As a 60x30 grid with 5 features per cell = 900x5\n","# Input batch will have size batch_size x 900x5\n","\n","# Using YOLO like architecture\n","# 9 convolutoion layers and one fully connected layer\n","# Kernel for convolution = 3x3, stride = 1\n","# Kernel for MaxPooling = 2x2, stride = 2\n","\n","class CNN(nn.Module):\n","  \n","  def __init__(self):\n","    super(CNN,self).__init__()\n","    \n","    # Convolution layer 1\n","    # Out channels is the number of filters\n","    self.conv1 = nn.Conv2d(in_channels=3, out_channels=16,\n","                           kernel_size=3, padding =1)\n","    self.bn1 = nn.BatchNorm2d(16)\n","    \n","    # Convolution layer 2\n","    self.conv2 = nn.Conv2d(in_channels=16, out_channels=32,\n","                           kernel_size=3, padding =1)\n","    self.bn2 = nn.BatchNorm2d(32)\n","    \n","    # Convolution layer 3\n","    self.conv3 = nn.Conv2d(in_channels=32, out_channels=64,\n","                           kernel_size=3, padding =1)\n","    self.bn3 = nn.BatchNorm2d(64)\n","    \n","    # Convolution layer 4\n","    self.conv4 = nn.Conv2d(in_channels=64, out_channels=128,\n","                           kernel_size=3, padding =1)\n","    self.bn4 = nn.BatchNorm2d(128)\n","    \n","    # Convolution layer 5\n","    self.conv5 = nn.Conv2d(in_channels=128, out_channels=256,\n","                           kernel_size=3, padding =1)\n","    self.bn5 = nn.BatchNorm2d(256)\n","    \n","    # Convolution layer 6\n","    self.conv6 = nn.Conv2d(in_channels=256, out_channels=512,\n","                           kernel_size=3, padding =1)\n","    self.bn6 = nn.BatchNorm2d(512)\n","    \n","    # Convolution layer 7\n","    self.conv7 = nn.Conv2d(in_channels=512, out_channels=1024,\n","                           kernel_size=3, padding =1)\n","    self.bn7 = nn.BatchNorm2d(1024)\n","    \n","    # Convolution layer 8\n","    self.conv8 = nn.Conv2d(in_channels=1024, out_channels=1024,\n","                           kernel_size=3, padding =1)\n","    self.bn8 = nn.BatchNorm2d(1024)\n","    \n","    # Convolution layer 9\n","    self.conv9 = nn.Conv2d(in_channels=1024, out_channels=1024,\n","                           kernel_size=3, padding =1)\n","        \n","    \n","    # Fully connected Layer\n","    self.fc1 = nn.Linear(in_features=82944, out_features=30*30*5)\n","    \n","    # Now we get \n","    \n","  def forward(self,x):\n","    x= self.bn1(self.conv1(x))\n","    x= F.max_pool2d(x,kernel_size=2,stride=2)\n","    x= F.leaky_relu(x,inplace=False)\n","    \n","    x= self.bn2(self.conv2(x))\n","    x= F.max_pool2d(x,kernel_size=2,stride=2)\n","    x= F.leaky_relu(x,inplace=False)\n","    \n","    x= self.bn3(self.conv3(x))\n","    x= F.max_pool2d(x,kernel_size=2,stride=2)\n","    x= F.leaky_relu(x,inplace=False)\n","    \n","    x= self.bn4(self.conv4(x))\n","    x= F.max_pool2d(x,kernel_size=2,stride=2)\n","    x= F.leaky_relu(x,inplace=False)\n","    \n","    x= self.bn5(self.conv5(x))\n","    x= F.max_pool2d(x,kernel_size=2,stride=2)\n","    x= F.leaky_relu(x,inplace=False)\n","    \n","    x= self.bn6(self.conv6(x))\n","    x= F.max_pool2d(x,kernel_size=2,stride=2)\n","    x= F.leaky_relu(x,inplace=False)\n","    \n","    \n","    x= self.bn7(self.conv7(x))\n","    x= self.bn8(self.conv8(x))\n","    x= self.conv9(x)\n","    \n","    # Flattening the output before it hits fully connected layer\n","    x = x.view(x.size(0), -1) \n","    #x= x.view(-1) \n","    x= self.fc1(x)\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"48SExk-7bZg1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"356caa13-7060-43a0-84f4-5aa1cf238ff8","executionInfo":{"status":"ok","timestamp":1534978755484,"user_tz":240,"elapsed":9127,"user":{"displayName":"Soumya Mohanty","photoUrl":"//lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s50-c-k-no/photo.jpg","userId":"105930513908317089321"}}},"cell_type":"code","source":["# Setting the device that will be used for training.\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","# Creating the model instance. \n","model = CNN().to(device)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"metadata":{"id":"50FmwhsX0ZpF","colab_type":"code","colab":{}},"cell_type":"code","source":["# Loss function and optimizer\n","\n","criterion = nn.L1Loss() # Loss Function\n","\n","# We are using Stochastic Gradient descent as our optimizer\n","optimizer = optim.SGD(model.parameters(), lr=0.01)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Iiq2CuMs0ci6","colab_type":"code","colab":{}},"cell_type":"code","source":["# Training loop\n","\n","def train(epochs):\n","  model.train()\n","  \n","  for epoch in range(epochs):\n","    \n","    \n","    # As dataloader has batch_size = 4\n","    # We will get one batch of 4 images with each iteration \n","    for i, (images_batch, target) in enumerate(train_dataloader):\n","\n","      # Convert images_batch and target to pytorch Variable\n","      images_batch, target_batch = Variable(images_batch),Variable(target)\n","\n","      optimizer.zero_grad() # Make sure gradients are initially 0\n","      \n","      # Converting input to type torch.cuda\n","      images_batch.requires_grad_()\n","      images_batch = images_batch.to(device)\n","      \n","      # Converting targets to type torch.cuda\n","      #target.requires_grad_()\n","      target = target.to(device)\n","      \n","      out = model(images_batch) # Forward pass\n","\n","      loss = criterion(out, target) # Computing the loss\n","      loss.backward() # Back-Prop the loss / Backward Pass\n","\n","      optimizer.step() # Update the gradients\n","      \n","      print(f'Batch : {i+1} Loss : {loss}')\n","\n","    print(f'Epoch : {epoch+1} Loss : {loss}')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nGiThOhBdI7K","colab_type":"code","colab":{}},"cell_type":"code","source":["train(200)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dY5nS9R90ebc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"dfc0182e-8962-407a-a5fe-b63bfb2203ce","executionInfo":{"status":"ok","timestamp":1534976307089,"user_tz":240,"elapsed":985,"user":{"displayName":"Soumya Mohanty","photoUrl":"//lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s50-c-k-no/photo.jpg","userId":"105930513908317089321"}}},"cell_type":"code","source":["# Insert zip folder with all validation images in it. Then unzip it\n","fileId = drive.CreateFile({'id': '1SF7FfUuuB5ragXpFZf4a0KP8xZsCoc6w'}) # id is the id of the file being uploaded\n","print (fileId['title'])  # dataset_test.zip\n","file = fileId['title']\n","fileId.GetContentFile(file)  # Save Drive file as a local file\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["dataset_test.zip\n"],"name":"stdout"}]},{"metadata":{"id":"HjmLU7FN0h9P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"0f3f6a58-be37-4f19-df2f-02a224739a83","executionInfo":{"status":"ok","timestamp":1534976311190,"user_tz":240,"elapsed":1448,"user":{"displayName":"Soumya Mohanty","photoUrl":"//lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s50-c-k-no/photo.jpg","userId":"105930513908317089321"}}},"cell_type":"code","source":["# Unzip the datafile\n","!unzip dataset_test.zip -d ./"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Archive:  dataset_test.zip\r\n","   creating: ./dataset_test/\r\n"],"name":"stdout"}]},{"metadata":{"id":"2lTe-GhTr4-U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"4f9d8f77-b933-4116-dfbc-95c295d9aa58","executionInfo":{"status":"ok","timestamp":1534976319475,"user_tz":240,"elapsed":3970,"user":{"displayName":"Soumya Mohanty","photoUrl":"//lh5.googleusercontent.com/-FoKo95MUut8/AAAAAAAAAAI/AAAAAAAAAAg/K76D68CSnok/s50-c-k-no/photo.jpg","userId":"105930513908317089321"}}},"cell_type":"code","source":["!ls\n","!rm dataset_test.zip\n","!ls"],"execution_count":17,"outputs":[{"output_type":"stream","text":["adc.json  datalab  dataset_test  dataset_test.zip  dataset_train  sample_data\n","adc.json  datalab  dataset_test  dataset_train\tsample_data\n"],"name":"stdout"}]},{"metadata":{"id":"eG2HlFrQr7bn","colab_type":"code","colab":{}},"cell_type":"code","source":["# Loading the validation set\n","\n","# Normalizing the images\n","normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n","                                 std=[0.229, 0.224, 0.225])\n","\n","# creating transform, resizing again (can be removed)\n","transform = transforms.Compose([transforms.Resize((600,600)),\n","                                transforms.ToTensor(),\n","                                normalize])\n","\n","# Creating dataset object\n","val_dataset = ImgDataset('dataset_test', transform = transform, \n","                 img_shape = (600,600), grid_dim = (30,30))\n","\n","# Creating dataloader to load a batch of 1 images at once, as the number of images in val set keep changing\n","# Fix it to 4 or 8 once the val set is fixed.\n","val_dataloader = DataLoader(train_dataset,\n","                              batch_size=1, \n","                              shuffle=True,\n","                              num_workers=0,\n","                              pin_memory=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EszjKhYwsWf7","colab_type":"code","colab":{}},"cell_type":"code","source":["# Validating the model\n","\n","def validate(epochs):\n","  model.eval()\n","  loss = 0\n","  correct = 0\n","  \n","  for epoch in range(epochs):\n","    \n","    \n","    # As dataloader has batch_size = 4\n","    # We will get one batch of 4 images with each iteration \n","    for i, (images_batch, target) in enumerate(val_dataloader):\n","\n","      # Convert images_batch and target to pytorch Variable\n","      images_batch, target = Variable(images_batch),Variable(target)\n","\n","      # setting this context handler ensures that the loss is not\n","      # backpropagated and we don't run out of GPU memory.\n","      # Note: if we still run out of GPU memory then decrease batch size\n","      # in the dataLoader (val_loder in this case)\n","      \n","      with  torch.no_grad():\n","        # Converting input to type torch.cuda\n","        images_batch.requires_grad_()\n","        images_batch = images_batch.to(device)\n","\n","        # Converting targets to type torch.cuda\n","        target.requires_grad_()\n","        target = target.to(device)\n","\n","        out = model(images_batch) # Forward pass\n","\n","        loss += criterion(out, target) # Computing the loss and summing over it\n","\n","        print(f'Batch : {i+1} Aggregate batch loss : {loss}')\n","        \n","        pred = out.data\n","        \n","        correct += pred.eq(target.data).sum()\n","        \n","    # Computing average loss    \n","    loss /= len(val_dataloader.dataset)\n","        \n","    print(f'\\nAverage loss: {loss}, Accuracy: {correct}/{len(val_dataloader.dataset)},{100*(correct/len(val_dataloader.dataset))}')\n","    \n","    print(f'Epoch : {epoch+1} avg loss : {loss}')\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Os3ERCHOtSTd","colab_type":"code","colab":{}},"cell_type":"code","source":["validate(1) # As we dont need more than one epoch for validating"],"execution_count":0,"outputs":[]}]}